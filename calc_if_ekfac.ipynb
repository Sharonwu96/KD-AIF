{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad005bc9-c38c-4380-b79a-cba6a9236667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "\n",
    "#from examples.cifar.pipeline import get_cifar10_dataset\n",
    "from kronfluence.analyzer import Analyzer, prepare_model\n",
    "from kronfluence.arguments import FactorArguments\n",
    "from kronfluence.task import Task\n",
    "from kronfluence.utils.dataset import DataLoaderKwargs\n",
    "\n",
    "BATCH_TYPE = Tuple[torch.Tensor, torch.Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5341dc91-2856-451c-aaf1-090fd38bb3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "transforms.Resize(224),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "#采用自带的Cifar100\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data_CIFAR100', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    " \n",
    "testset = torchvision.datasets.CIFAR100(root='./data_CIFAR100', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a80765-7a16-409f-b584-e6f0e5720e76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 113, 226, 235, 377, 469, 484, 614, 623, 655, 132, 267, 572, 598, 661, 674, 682, 713, 725, 795, 54, 216, 325, 414, 434, 463, 504, 542, 573, 887, 396, 413, 493, 596, 633, 737, 965, 1028, 1055, 1176, 50, 140, 159, 172, 248, 478, 607, 754, 1105, 1312, 334, 441, 459, 657, 1142, 1206, 1219, 1272, 1274, 1314, 51, 83, 200, 204, 258, 268, 381, 404, 468, 535, 64, 198, 303, 329, 347, 376, 458, 586, 648, 686, 27, 28, 116, 161, 319, 417, 476, 532, 554, 644, 52, 114, 170, 340, 455, 575, 594, 864, 932, 997]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_sample_ids_per_class(class_id, num_samples, test_dataset,\n",
    "                                     start_index=0):\n",
    "    sample_list = []\n",
    "    img_count = 0\n",
    "    for i in range(len(test_dataset)):\n",
    "        _, t = test_dataset[i]\n",
    "        if class_id == t:\n",
    "            img_count += 1\n",
    "            if (img_count > start_index) and \\\n",
    "                    (img_count <= start_index + num_samples):\n",
    "                sample_list.append(i)\n",
    "            elif img_count > start_index + num_samples:\n",
    "                break\n",
    "    return sample_list\n",
    "\n",
    "def get_dataset_sample_ids(num_samples, test_dataset, num_classes=10,\n",
    "                           start_index=0):\n",
    "\n",
    "    sample_dict = {}\n",
    "    sample_list = []\n",
    "    if start_index > len(test_dataset) / num_classes:\n",
    "        logging.warn(f\"The variable test_start_index={start_index} is \"\n",
    "                     f\"larger than the number of available samples per class.\")\n",
    "    for i in range(num_classes):\n",
    "        sample_dict[str(i)] = get_dataset_sample_ids_per_class(\n",
    "            i, num_samples, test_dataset, start_index)\n",
    "        sample_list[len(sample_list):len(sample_list)] = sample_dict[str(i)]\n",
    "    return sample_dict, sample_list\n",
    "\n",
    "sample_dict, sample_list = get_dataset_sample_ids(10, testset, num_classes=10,\n",
    "                           start_index=0)\n",
    "\n",
    "all_values = []\n",
    "for value in sample_dict.values():\n",
    "    all_values.extend(value)\n",
    "\n",
    "print(all_values)\n",
    "print(len(all_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b94ad8-e855-4327-9541-b7e1432901d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrupt_percentage = None\n",
    "dataset_dir = \"./data_CIFAR100\"\n",
    "checkpoint_dir = \"./\"\n",
    "query_batch_size = 100\n",
    "factor_strategy = \"ekfac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24dc2bae-c493-4a95-b215-7dd37fbdec67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationTask(Task):\n",
    "    def compute_train_loss(\n",
    "        self,\n",
    "        batch: BATCH_TYPE,\n",
    "        model: nn.Module,\n",
    "        sample: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        inputs, labels = batch\n",
    "        logits = model(inputs)\n",
    "        if not sample:\n",
    "            return F.cross_entropy(logits, labels, reduction=\"sum\")\n",
    "        with torch.no_grad():\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            sampled_labels = torch.multinomial(\n",
    "                probs,\n",
    "                num_samples=1,\n",
    "            ).flatten()\n",
    "        return F.cross_entropy(logits, sampled_labels.detach(), reduction=\"sum\")\n",
    "\n",
    "    def compute_measurement(\n",
    "        self,\n",
    "        batch: BATCH_TYPE,\n",
    "        model: nn.Module,\n",
    "    ) -> torch.Tensor:\n",
    "        # Copied from: https://github.com/MadryLab/trak/blob/main/trak/modelout_functions.py.\n",
    "        inputs, labels = batch\n",
    "        logits = model(inputs)\n",
    "\n",
    "        bindex = torch.arange(logits.shape[0]).to(device=logits.device, non_blocking=False)\n",
    "        logits_correct = logits[bindex, labels]\n",
    "\n",
    "        cloned_logits = logits.clone()\n",
    "        cloned_logits[bindex, labels] = torch.tensor(-torch.inf, device=logits.device, dtype=logits.dtype)\n",
    "\n",
    "        margins = logits_correct - cloned_logits.logsumexp(dim=-1)\n",
    "        return -margins.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e5253-9b91-4c89-9411-b2c35d4fc02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d4e1c98-5a34-4840-baea-c4686a9b5b4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=models.mobilenet_v2(pretrained=False)\n",
    "\n",
    "model.classifier = nn.Sequential( \n",
    "    #重新定义特征层，根据需要可以添加自己想要的Linear层\n",
    "    nn.Dropout(p=0.2, inplace=False),\n",
    "    nn.Linear(in_features=1280, out_features=100),  #多加几层都没关系\n",
    "    #nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "student_save_path='./Dis_resnet50(T)_mobilebetv2(S)_cifar100_epoch10_withif_1.pkl'\n",
    "model.load_state_dict(torch.load(student_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc3c916-04f2-4694-aecb-21426a9f401b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define task and prepare model.\n",
    "task = ClassificationTask()\n",
    "model = prepare_model(model, task)\n",
    "\n",
    "analyzer = Analyzer(\n",
    "    analysis_name=\"cifar100\",\n",
    "    model=model,\n",
    "    task=task,\n",
    ")\n",
    "# Configure parameters for DataLoader.\n",
    "dataloader_kwargs = DataLoaderKwargs(num_workers=1)\n",
    "analyzer.set_dataloader_kwargs(dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "264436b5-2aac-43a0-abf0-da2b800ae4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compute influence factors.\n",
    "factor_args = FactorArguments(strategy=factor_strategy)\n",
    "analyzer.fit_all_factors(\n",
    "    factors_name=factor_strategy,\n",
    "    dataset=trainset,\n",
    "    per_device_batch_size=None,\n",
    "    factor_args=factor_args,\n",
    "    overwrite_output_dir=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93f0e87-ed0a-4e9b-94e3-a5b7dad768e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_modules': tensor([[-8.9190e+02, -8.7845e-04,  7.8836e+05,  ..., -1.4713e+04,\n",
       "           2.4061e+04, -9.9291e+01],\n",
       "         [-3.4495e+02,  3.0169e-04,  3.1101e+05,  ...,  5.6217e+03,\n",
       "           1.2414e+04,  9.1736e+01],\n",
       "         [ 3.7183e+03, -1.8812e-03,  5.0622e+05,  ...,  2.0623e+04,\n",
       "           2.0881e+04,  5.2695e+01],\n",
       "         ...,\n",
       "         [-5.9469e+02,  7.2639e-04, -2.7661e+04,  ...,  9.9768e+03,\n",
       "           2.7745e+04, -1.0639e+02],\n",
       "         [ 3.7473e+03, -1.0781e-03, -2.6129e+04,  ...,  4.4032e+03,\n",
       "          -4.5703e+04,  1.4691e+02],\n",
       "         [-6.3104e+03,  5.0711e-03,  7.4715e+03,  ..., -8.9762e+02,\n",
       "          -1.2028e+04, -1.2531e+03]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Compute pairwise scores.\n",
    "analyzer.compute_pairwise_scores(\n",
    "    scores_name=factor_strategy,\n",
    "    factors_name=factor_strategy,\n",
    "    query_dataset=testset,\n",
    "    #query_indices=list(range(2000)),\n",
    "    query_indices=all_values,\n",
    "    train_dataset=trainset,\n",
    "    per_device_query_batch_size=query_batch_size,\n",
    "    overwrite_output_dir=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0395a88c-8334-4c52-9c89-9d7bd321c629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 50000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = analyzer.load_pairwise_scores(factor_strategy)[\"all_modules\"]\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30eb6182-92eb-4824-ac29-c7a4ffc405ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean() received an invalid combination of arguments - got (dtype=NoneType, out=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m score_test_mean_1 \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m#每个training data将对应的100个test data原始分数的平均分数\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m----> 5\u001b[0m     score_test_mean_1\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(score_test_mean_1)) \u001b[38;5;66;03m#长度为50000的list\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3502\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3500\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   3501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   3505\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: mean() received an invalid combination of arguments - got (dtype=NoneType, out=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "score_test_mean_1 = [] #每个training data将对应的100个test data原始分数的平均分数\n",
    "\n",
    "for i in range(scores.shape[1]):\n",
    "    score_test_mean_1.append(np.mean(scores[:, i]))\n",
    "\n",
    "print(len(score_test_mean_1)) #长度为50000的list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc9d43d-9776-4ed2-a28b-e7ff998276f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feebccc9-ee2b-46a1-bb20-593ac096fff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
